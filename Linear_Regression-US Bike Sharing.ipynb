#!/usr/bin/env python
# coding: utf-8

# # Problem Statement :
# #### A US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. They aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends.They have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:
# 
# #### Essentially the Company wants :
# 1. Which variables are significant in predicting the demand for shared bikes?
# 2. How well those variables describe the bike demands?
# 
# #### So interpretation is important !!
# 

# ## The steps we will perform in this exercise are as follow :- 
# 
# 1. Reading and understanding the data
# 2. Preparing the data
# 3. Training the model
# 4. Residual Analysis
# 5. Predictions and Evaluation on test set
# 6. Final conclusion and predictions
# 7. The best fit line

# ## Importing Important Libraries 

# In[901]:


import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

import sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

import warnings 
warnings.filterwarnings('ignore')


# ##### Step 1 : Reading and understanding the data

# In[902]:


df=pd.read_csv('day.csv')
df.head()


# In[903]:


#checking missing values and datatypes
df.info()


# In[904]:


df.isnull().sum()


# ## - No missing Values or suspicious data types

# In[905]:


#how many number of columns and rows available in data set
df.shape


# In[906]:


#getting the deapth of data
df.describe()


# In[907]:


###### Removed redundant columns
#Instant is a index column, dteday has same information given in year and weekday. 
# The sum of casual and registered users has been calculated in Cnt, hence they are not useful. 
df.drop(['dteday','instant','casual','registered',], axis=1, inplace=True)


# In[908]:


#reduced number of columns just to cross check
df.shape


# In[909]:


#changing columns name to make them look more relevant 
df.rename(columns={ 'yr' : 'Year', 'mnth' : 'Month', 'temp' : 'temperature' , 'hum' : 'humidity', 'cnt': 'Total Rent_Bike'}, 
          inplace=True)
df.columns


# In[847]:


#some basic graphical representation before treating variables
sns.pairplot(df)
plt.show()


# In[910]:


sns.regplot(x='Total Rent_Bike',y='temperature', data=df)
#temprature and dependent variables seems to have a positive correlation


# In[911]:


sns.regplot(x='Total Rent_Bike',y='atemp', data=df)
#sniffing some multicollinearity between temprature and atemp, but will treat them later


# # checking outliers presence in data

# In[850]:


plt.figure(figsize=(20,12))
plt.subplot(2,3,1)
sns.boxplot(x='season', y='Total Rent_Bike',data=df)
plt.subplot(2,3,2)
sns.boxplot(x='Year', y='Total Rent_Bike',data=df)
plt.subplot(2,3,3)
sns.boxplot(x='Month', y='Total Rent_Bike',data=df)
plt.subplot(2,3,4)
sns.boxplot(x='holiday', y='Total Rent_Bike',data=df)
plt.subplot(2,3,5)
sns.boxplot(x='weekday', y='Total Rent_Bike',data=df)
plt.subplot(2,3,6)
sns.boxplot(x='workingday', y='Total Rent_Bike',data=df)


# ### No outliers in the data,therefore no treatment given and can also use standard scaling

# In[851]:


plt.figure(figsize=(20,12))
plt.subplot(2,2,1)
sns.boxplot(x='weathersit', y='Total Rent_Bike',data=df)
plt.subplot(2,2,2)
sns.boxplot(x='temperature', y='Total Rent_Bike',data=df)
plt.subplot(2,2,3)
sns.boxplot(x='humidity', y='Total Rent_Bike',data=df)
plt.subplot(2,2,4)
sns.boxplot(x='windspeed', y='Total Rent_Bike',data=df)


# In[852]:


plt.figure(figsize=(16,10))
sns.heatmap(df.corr(),annot=True,cmap='YlGnBu')
plt.show()
#to understand correlations with help of heatmap, could visibly notice temrature and atemp are multicollinear, hence dropping one


# In[853]:


#to check relationship between numerical variables with dependent variable
sns.pairplot(df,x_vars=['temperature','humidity','windspeed','Year'],
            y_vars=['Total Rent_Bike'],
            )
plt.show()


# In[912]:


#since temperature and atemp is highly co-related , dropping atemp
df.drop(['atemp'], axis=1, inplace=True)
df.head()


# ### Preparing the data for modelling 
# - Encoding
#     - coverting variables into categorical string as per given data dictionary
#     - Then converting them into dummies 
#     

# In[913]:


df['season']=df.season.map({1:'spring',2:'summer',3:'fall',4:'winter'})
df['Month']=df.Month.map({1:'January',2:'Feburary',3:'March',4:'April',5:'May',6:'June',7:'July',8:'August',
                       9:'September',10:'October',11:'November', 12:'December'})
df['weekday']=df.weekday.map({1:'Monday', 2: 'Tuesday', 3:'Wednesday', 4: 'Thursday', 5: 'Friday', 6: 'Saturday', 
                       0:'Sunday'})
df['weathersit']=df.weathersit.map({1: 'Clear', 2:'Mist', 3:'Light Snow',4:'Snow + Fog'})
df.head()


# In[914]:


df['weekday'].unique()


# In[915]:


df.weathersit.unique()


# In[916]:


df.Month.unique()


# In[917]:


df.season.unique()


# In[918]:


#analysing relation between season and target variable
sns.barplot('season','Total Rent_Bike',data=df)


# In[919]:


##analysing relation between Total Demand and target variable
plt.figure(figsize=(8,5))
plt.title("Total Demand Vs Month")
sns.barplot('Month','Total Rent_Bike',hue='Year',data=df,palette='Paired')
plt.xticks(rotation=45)
plt.show()


# In[920]:


plt.title("Weather Condition Vs Bike Demand")
sns.barplot('weathersit','Total Rent_Bike',data=df)
plt.show()


# In[921]:


plt.figure(figsize=(5,3))
plt.title("Weekday vs Bike Demand")
plt.xticks(rotation=50)
sns.barplot('weekday','Total Rent_Bike',data=df)
plt.show()


# In[922]:


months=pd.get_dummies(df.Month,drop_first=True)
weekday=pd.get_dummies(df.weekday,drop_first=True)
weathersit=pd.get_dummies(df.weathersit, drop_first=True)
season=pd.get_dummies(df.season, drop_first=True)


# In[923]:


df=pd.concat([months,weekday,weathersit,season,df],axis=1)
df.head()


# In[924]:


df.drop(['Month','weekday','weathersit','season'], axis=1, inplace=True)
df.shape


# In[925]:


df.columns


# In[926]:


#checking other variables and nature of values, this is binary variable
df.holiday.unique()


# In[927]:


#another binary variable
df.workingday.unique()


# In[928]:


df.shape


# In[929]:


df.head()


# # Splitting into train and test
# - Rescaling the variables

# In[930]:


# dividing 70% data into train and remaining 30% to test the analysis, make predictions
df_train, df_test=train_test_split(df,train_size=0.7,random_state=100)


# In[931]:


df_train.shape


# In[932]:


#although, we can use standard scaling also because there are no outliers in the data, but we preferred MinMax to put everything
#under 0 and 1
sacler=MinMaxScaler()
#list of numeric variable 
num_vars=['temperature' ,
          'humidity' , 'windspeed','Total Rent_Bike']

#fitting and transforming the variables

df_train[num_vars]=scaler.fit_transform(df_train[num_vars])
df_train.head()


# In[933]:


#extracting target variable to assign Y_train & remaining to X_train
y_train=df_train.pop('Total Rent_Bike')
X_train=df_train


# In[934]:


#using RFE approach for feature selection and using automatic approach of dropping variables 
lm=LinearRegression()
lm.fit(X_train,y_train)
rfe=RFE(lm,15)
rfe=rfe.fit(X_train,y_train)


# In[935]:


list(zip(X_train.columns,rfe.support_,rfe.ranking_))


# In[936]:


col=X_train.columns[rfe.support_]
col


# In[937]:


X_train.columns[~rfe.support_]


# In[938]:


X_train_rfe=X_train[col]


# In[939]:


#first model with 15 variables
X_train_rfe=sm.add_constant(X_train_rfe)
lm=sm.OLS(y_train,X_train_rfe).fit()
lm.summary()


# In[940]:


vif=pd.DataFrame()
vif['feature']=X_train_rfe.columns
vif['vif']=[variance_inflation_factor(X_train_rfe.values,i) for i in range(X_train_rfe.shape[1])]
vif['vif']=round(vif['vif'],2)
vif=vif.sort_values(by='vif',ascending=False)
vif


# ## Reducing Columns to 10 using RFE again

# In[948]:


lm=LinearRegression()
lm.fit(X_train,y_train)
rfe_new=RFE(lm,10)
rfe_new=rfe_new.fit(X_train,y_train)
list(zip(X_train.columns,rfe_new.support_,rfe_new.ranking_))


# In[949]:


col=X_train.columns[rfe_new.support_]
col


# In[950]:


X_train.columns[~rfe_new.support_]


# In[951]:


X_train_rfe_new=X_train[col]


# In[952]:


X_train_rfe_new=sm.add_constant(X_train_rfe_new)
lm=sm.OLS(y_train,X_train_rfe_new).fit()
lm.summary()


# In[ ]:





# # VIF
# ##### We could have
# - high P value, high VIF
# - high-low :
#     - high P, low VIF (Remove These First)
#     - Low p, high VIF (remove after first one)
# - Low P and low VIF (Keep it)
# 

# In[953]:


vif=pd.DataFrame()
vif['feature']=X_train_rfe_new.columns
vif['vif']=[variance_inflation_factor(X_train_rfe_new.values,i) for i in range(X_train_rfe_new.shape[1])]
vif['vif']=round(vif['vif'],2)
vif=vif.sort_values(by='vif',ascending=False)
vif


# #### Residual Analysis 

# In[954]:


y_train_pred=lm.predict(X_train_rfe_new)
fig=plt.figure()
sns.distplot((y_train-y_train_pred),bins=20)
fig.suptitle('Errors Terms',fontsize=15)
plt.xlabel('Errors', fontsize=13)
plt.show()


# ## Prediction and Evaluation on Test set
# - Applying the scaling on the test set

# In[955]:


num_vars=['temperature' ,
          'humidity' , 'windspeed','Total Rent_Bike']
df_test[num_vars]=scaler.transform(df_test[num_vars])
df_test.describe()


# In[956]:


y_test=df_test.pop('Total Rent_Bike')
X_test=df_test
X_test.head()


# In[957]:


y_test


# In[958]:


X_test_new=X_test[col]


# In[959]:


X_test_new=sm.add_constant(X_test_new)


# In[960]:


y_test_pred=lm.predict(X_test_new)


# ## Test Model Evaluation 

# In[961]:


# Plotting Actual Value Vs Predicted Values
ax1 = sns.distplot(y_test, hist=False, color="r", label="Actual Value")
sns.distplot(y_test_pred, hist=False, color="b", label="Predicted Values" , ax=ax1)
plt.legend()
plt.show()


# Inferences :
# Actual value and predicted value follow similar curve pattern.
# It shows that the model is good enough to predict the count of rentak bikes.

# In[962]:


Test_score=r2_score(y_true=y_test,y_pred=y_test_pred)
round(Test_score,4)


# In[963]:


#Calculate the r square for train
r_squared_train = r2_score(y_train, y_train_pred)
round(r_squared_train,4)


# #### Inference 
# - R2 model is 80% 

# In[964]:


Difference_bw_r2=0.834-Test_score
Difference_bw_r2
#it has to be less than 0.05 to confirm that it is a good model 


# In[965]:


from sklearn.metrics import r2_score
r2_score(y_test, y_test_pred)


# In[966]:


0.05-Difference_bw_r2


# #### Equation of best fitted line

# In[967]:


lm.summary()


# ##### Equation
# - cnt= (-0.0482 july)+(0.0944 September)+(-0.1914 Light Snow)+(0.0800 summer)+(0.1390winter)+(0.2259Year)+(-0.0930holiday)
#       +(0.6200temperature)+(-0.2868humidity)+(-0.2059 windspeed)
#       
# #### Count of rental bikes depend on following features. 
# - July
# - September
# - Light Snow
# - summer
# - winter
# - Year
# - holiday
# - temperature
# - humidity
# - windspeed

# In[ ]:




